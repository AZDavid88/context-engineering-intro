#!/usr/bin/env python3
"""
Phase 3 Ultra-Compressed Evolution Integration Tests

Comprehensive integration testing for the Phase 3 UltraCompressedEvolution
implementation, validating all components work together correctly following
the phase-executor methodology.

Test Coverage:
- Ultra-compressed evolution orchestrator functionality
- Triple validation pipeline integration
- Strategy deployment manager operations
- Resilience manager failure handling
- System health monitor comprehensive assessment
- PaperTradingEngine validation extensions
- Cross-component integration and data flow

This test suite follows CODEFARM systematic testing methodology to ensure
production-ready reliability and zero integration issues.
"""

import pytest
import asyncio
import logging
import time
import tempfile
import json
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Any

# Test imports - verified architecture patterns
from src.config.settings import get_settings
from src.strategy.genetic_seeds.base_seed import BaseSeed, SeedType, SeedGenes
from src.execution.paper_trading import PaperTradingEngine, PaperTradingMode
# Test utilities for mock data (moved from production)
from tests.utils.market_data_fixtures import create_test_market_data

# Phase 3 component imports
from scripts.evolution.ultra_compressed_evolution import UltraCompressedEvolution, UltraEvolutionConfig
from src.validation.triple_validation_pipeline import TripleValidationPipeline, ValidationMode
from src.execution.strategy_deployment_manager import StrategyDeploymentManager, DeploymentMode
from src.execution.resilience_manager import ResilienceManager, FailureType, CircuitBreaker, CircuitBreakerConfig
from src.monitoring.system_health_monitor import SystemHealthMonitor, HealthStatus

logger = logging.getLogger(__name__)


# Mock strategy class for testing
class MockStrategy(BaseSeed):
    """Mock strategy for testing purposes."""
    
    def __init__(self, fitness_score: float = 1.0, config_name: str = "mock_strategy"):
        # Create minimal genes for BaseSeed with required fields
        genes = SeedGenes(
            seed_id=f"mock_seed_{config_name}",
            seed_type=SeedType.MOMENTUM,
            parameters={"lookback_period": 20.0}  # Add required parameter
        )
        super().__init__(genes)
        
        self.fitness = fitness_score
        self._config_name = config_name
        self._validation_score = fitness_score * 0.9
    
    @property
    def seed_name(self) -> str:
        """Return human-readable seed name."""
        return self._config_name
    
    @property
    def seed_description(self) -> str:
        """Return seed description."""
        return f"Mock strategy for testing with fitness {self.fitness}"
    
    @property
    def required_parameters(self) -> List[str]:
        """Return required parameters."""
        return ["lookback_period"]
    
    @property
    def parameter_bounds(self) -> Dict[str, tuple]:
        """Return parameter bounds."""
        return {"lookback_period": (5, 50)}
    
    def calculate_technical_indicators(self, data) -> Dict[str, Any]:
        """Mock technical indicator calculation."""
        return {"mock_indicator": [1.0] * len(data)}
    
    async def generate_signals(self, data, filtered_assets=None, current_asset=None, timeframe='1h'):
        """Mock signal generation."""
        import pandas as pd
        if isinstance(data, pd.DataFrame):
            signals = pd.Series([1] * len(data), index=data.index)
        else:
            signals = {"signal": "buy", "confidence": 0.8}
        return signals


class TestPhase3IntegrationSuite:
    """Comprehensive Phase 3 integration test suite."""
    
    @pytest.fixture
    def settings(self):
        """Get test settings."""
        return get_settings()
    
    @pytest.fixture
    def mock_strategies(self):
        """Create mock strategies for testing."""
        return [
            MockStrategy(fitness_score=2.5, config_name="high_performance_strategy"),
            MockStrategy(fitness_score=1.8, config_name="medium_performance_strategy"), 
            MockStrategy(fitness_score=1.2, config_name="low_performance_strategy"),
            MockStrategy(fitness_score=0.8, config_name="poor_performance_strategy"),
            MockStrategy(fitness_score=1.5, config_name="average_performance_strategy")
        ]
    
    @pytest.fixture
    def ultra_evolution_config(self):
        """Create test configuration for ultra-compressed evolution."""
        return UltraEvolutionConfig(
            total_strategies=20,  # Reduced for testing
            target_hours=0.5,    # 30 minutes for testing
            batch_size=5,
            generations_per_batch=3,
            validation_mode="fast"
        )
    
    async def test_ultra_compressed_evolution_orchestrator(self, ultra_evolution_config, settings):
        """Test the main ultra-compressed evolution orchestrator."""
        
        logger.info("ğŸ§ª Testing Ultra-Compressed Evolution Orchestrator")
        
        # Initialize orchestrator
        evolution_system = UltraCompressedEvolution(ultra_evolution_config, settings)
        
        # Test initialization
        assert evolution_system.config.total_strategies == 20
        assert evolution_system.config.target_hours == 0.5
        assert evolution_system.genetic_pool is not None
        assert evolution_system.config_loader is not None
        assert evolution_system.decision_engine is not None
        
        logger.info("âœ… Orchestrator initialization successful")
        
        # Test fallback evolution (without Ray for testing) - now requires market data
        test_market_data = create_test_market_data(days=30, seed=42)
        evolution_results = await evolution_system._execute_local_evolution_fallback(test_market_data)
        
        assert evolution_results["total_strategies"] > 0
        assert evolution_results["successful_batches"] >= 0
        assert evolution_results["evolution_time_seconds"] > 0
        
        logger.info(f"âœ… Local evolution fallback: {evolution_results['total_strategies']} strategies generated")
        
        # Test serialization and loading
        config_results = await evolution_system._serialize_and_load_strategies(evolution_results)
        
        assert config_results["total_saved"] >= 0
        assert config_results["top_loaded"] >= 0
        
        logger.info(f"âœ… Strategy serialization: {config_results['total_saved']} saved, {config_results['top_loaded']} loaded")
        
        logger.info("ğŸ‰ Ultra-Compressed Evolution Orchestrator tests passed")
    
    async def test_triple_validation_pipeline(self, mock_strategies, settings):
        """Test the triple validation pipeline functionality."""
        
        logger.info("ğŸ§ª Testing Triple Validation Pipeline")
        
        # Initialize validation pipeline
        validation_pipeline = TripleValidationPipeline(settings=settings)
        
        # Test initialization
        assert validation_pipeline.backtesting_engine is not None
        assert validation_pipeline.paper_trading is not None
        assert validation_pipeline.performance_analyzer is not None
        assert validation_pipeline.validation_thresholds is not None
        
        logger.info("âœ… Validation pipeline initialization successful")
        
        # Test strategy validation with minimal mode for speed
        validation_results = await validation_pipeline.validate_strategies(
            strategies=mock_strategies[:3],  # Test with 3 strategies
            validation_mode="minimal",
            time_limit_hours=0.1,  # 6 minutes for testing
            concurrent_limit=2
        )
        
        # Verify results structure
        assert "strategies_validated" in validation_results
        assert "strategies_completed" in validation_results
        assert "strategies_passed" in validation_results
        assert "individual_results" in validation_results
        assert "aggregate_statistics" in validation_results
        
        # Verify at least some strategies were processed
        assert validation_results["strategies_validated"] == 3
        assert validation_results["strategies_completed"] >= 0
        assert validation_results["total_validation_time"] > 0
        
        # Verify individual results structure
        if validation_results["individual_results"]:
            individual_result = validation_results["individual_results"][0]
            assert "strategy_name" in individual_result
            assert "backtest" in individual_result
            assert "overall" in individual_result
        
        logger.info(f"âœ… Triple validation: {validation_results['strategies_completed']}/{validation_results['strategies_validated']} strategies completed")
        
        # Test validation summary
        summary = validation_pipeline.get_validation_summary(hours_back=1)
        assert "total_strategies" in summary
        
        logger.info("ğŸ‰ Triple Validation Pipeline tests passed")
    
    async def test_strategy_deployment_manager(self, mock_strategies, settings):
        """Test the strategy deployment manager functionality."""
        
        logger.info("ğŸ§ª Testing Strategy Deployment Manager")
        
        # Initialize deployment manager
        deployment_manager = StrategyDeploymentManager(settings=settings)
        
        # Test initialization
        assert deployment_manager.decision_engine is not None
        assert deployment_manager.paper_trading is not None
        assert deployment_manager.config_loader is not None
        assert deployment_manager.alerting is not None
        
        logger.info("âœ… Deployment manager initialization successful")
        
        # Test strategy deployment
        deployment_results = await deployment_manager.deploy_strategies(
            strategies=mock_strategies[:2],  # Test with 2 strategies
            deployment_mode=DeploymentMode.PAPER_TRADING
        )
        
        # Verify results structure - updated for consistent API
        assert "strategies_considered" in deployment_results
        assert "strategies_selected" in deployment_results
        assert "strategies_deployed" in deployment_results
        assert "deployment_failures" in deployment_results  # New field
        assert "deployment_records" in deployment_results
        assert "resource_allocation" in deployment_results  # New field
        
        # Verify deployment processing
        assert deployment_results["strategies_considered"] == 2
        assert deployment_results["strategies_selected"] >= 0
        assert deployment_results["strategies_deployed"] >= 0
        assert deployment_results["deployment_failures"] >= 0  # New field
        
        logger.info(f"âœ… Strategy deployment: {deployment_results['strategies_deployed']}/{deployment_results['strategies_considered']} deployed")
        
        # Test deployment summary
        summary = deployment_manager.get_deployment_summary()
        assert "active_deployments" in summary
        assert "total_deployed" in summary
        
        logger.info("ğŸ‰ Strategy Deployment Manager tests passed")
    
    async def test_resilience_manager(self, settings):
        """Test the resilience manager functionality."""
        
        logger.info("ğŸ§ª Testing Resilience Manager")
        
        # Initialize resilience manager
        resilience_manager = ResilienceManager(settings=settings)
        
        # Test initialization
        assert resilience_manager.resource_manager is not None
        assert resilience_manager.risk_manager is not None
        assert resilience_manager.alerting is not None
        assert resilience_manager.decision_engine is not None
        assert len(resilience_manager.circuit_breakers) > 0
        
        logger.info("âœ… Resilience manager initialization successful")
        
        # Test circuit breaker functionality
        test_breaker = CircuitBreaker("test_circuit", CircuitBreakerConfig(
            failure_threshold=2,
            recovery_timeout_seconds=1
        ))
        
        # Test successful operations
        async def successful_operation():
            return "success"
        
        result = await test_breaker.call(successful_operation)
        assert result == "success"
        assert test_breaker.state.value == "closed"
        
        logger.info("âœ… Circuit breaker success case passed")
        
        # Test resilient operation execution
        async def test_operation():
            return {"result": "test_completed", "value": 42}
        
        result, success = await resilience_manager.execute_with_resilience(
            operation=test_operation,
            operation_name="test_operation",
            failure_type=FailureType.COMPUTATION_FAILURE,
            timeout_seconds=5.0
        )
        
        assert success is True
        assert result["result"] == "test_completed"
        
        logger.info("âœ… Resilient operation execution passed")
        
        # Test health check
        health_results = await resilience_manager.perform_health_check()
        
        assert "circuit_breakers" in health_results
        assert "recent_failures" in health_results
        assert "resilience_state" in health_results
        assert "overall_healthy" in health_results
        
        logger.info(f"âœ… Resilience health check: {health_results['overall_healthy']}")
        
        # Test metrics
        metrics = resilience_manager.get_resilience_metrics()
        assert "resilience_score" in metrics
        assert "system_availability" in metrics
        
        logger.info("ğŸ‰ Resilience Manager tests passed")
    
    async def test_system_health_monitor(self, settings):
        """Test the system health monitor functionality."""
        
        logger.info("ğŸ§ª Testing System Health Monitor")
        
        # Initialize health monitor
        health_monitor = SystemHealthMonitor(settings=settings)
        
        # Test initialization
        assert health_monitor.monitoring_system is not None
        assert health_monitor.alerting is not None
        assert len(health_monitor.health_thresholds) > 0
        
        logger.info("âœ… Health monitor initialization successful")
        
        # Test comprehensive health check
        health_snapshot = await health_monitor.get_system_health()
        
        # Verify health snapshot structure
        assert health_snapshot.timestamp is not None
        assert hasattr(health_snapshot, 'overall_status')
        assert hasattr(health_snapshot, 'overall_score')
        assert hasattr(health_snapshot, 'components')
        
        # Verify core components are checked
        component_names = list(health_snapshot.components.keys())
        expected_components = ['system_resources', 'genetic_evolution', 'validation_pipeline']
        
        for expected in expected_components:
            assert expected in component_names, f"Missing component: {expected}"
        
        logger.info(f"âœ… Health check completed: {health_snapshot.overall_status.value} (score: {health_snapshot.overall_score:.3f})")
        
        # Test health summary
        summary = health_monitor.get_health_summary(hours_back=1)
        assert "snapshots" in summary
        assert "average_health_score" in summary
        
        # Test custom health checker registration
        async def custom_health_check():
            return {"healthy": True, "test_metric": 42}
        
        await health_monitor.register_component_checker("test_component", custom_health_check)
        
        # Run health check again to include custom checker
        health_snapshot_with_custom = await health_monitor.get_system_health()
        assert "test_component" in health_snapshot_with_custom.components
        
        logger.info("âœ… Custom health checker registration successful")
        
        logger.info("ğŸ‰ System Health Monitor tests passed")
    
    async def test_paper_trading_extensions(self, mock_strategies, settings):
        """Test the PaperTradingEngine validation extensions."""
        
        logger.info("ğŸ§ª Testing PaperTradingEngine Validation Extensions")
        
        # Initialize paper trading engine
        paper_trading = PaperTradingEngine(settings=settings)
        
        # Test accelerated replay validation
        strategy = mock_strategies[0]
        
        replay_results = await paper_trading.run_accelerated_replay(
            strategy=strategy,
            replay_days=30,
            acceleration_factor=10.0,
            mode=PaperTradingMode.ACCELERATED_REPLAY
        )
        
        # Verify replay results structure
        assert replay_results["success"] is True
        assert "performance_metrics" in replay_results
        assert "execution_quality" in replay_results
        assert "consistency_analysis" in replay_results
        
        # Verify performance metrics
        performance = replay_results["performance_metrics"]
        assert "total_trades" in performance
        assert "simulated_sharpe" in performance
        assert "simulated_returns" in performance
        
        logger.info(f"âœ… Accelerated replay: {performance['total_trades']} trades, Sharpe: {performance['simulated_sharpe']:.3f}")
        
        # Test testnet validation
        testnet_results = await paper_trading.deploy_testnet_validation(
            strategy=strategy,
            validation_hours=0.1,  # 6 minutes for testing
            mode=PaperTradingMode.LIVE_TESTNET
        )
        
        # Verify testnet results structure
        assert testnet_results["success"] is True
        assert "live_performance" in testnet_results
        assert "execution_analysis" in testnet_results
        assert "market_conditions" in testnet_results
        assert "risk_assessment" in testnet_results
        
        # Verify live performance metrics
        live_performance = testnet_results["live_performance"]
        assert "total_trades" in live_performance
        assert "performance_score" in live_performance
        assert "validation_duration_hours" in live_performance
        
        logger.info(f"âœ… Testnet validation: {live_performance['total_trades']} trades, performance: {live_performance['performance_score']:.3f}")
        
        logger.info("ğŸ‰ PaperTradingEngine Extensions tests passed")
    
    async def test_cross_component_integration(self, mock_strategies, settings, ultra_evolution_config):
        """Test integration between all Phase 3 components."""
        
        logger.info("ğŸ§ª Testing Cross-Component Integration")
        
        # Initialize all components
        evolution_system = UltraCompressedEvolution(ultra_evolution_config, settings)
        validation_pipeline = TripleValidationPipeline(settings=settings)
        deployment_manager = StrategyDeploymentManager(settings=settings)
        resilience_manager = ResilienceManager(settings=settings)
        health_monitor = SystemHealthMonitor(settings=settings)
        
        logger.info("âœ… All components initialized")
        
        # Test data flow: Evolution â†’ Validation â†’ Deployment
        
        # Step 1: Simulate evolution results
        mock_evolution_results = {
            "total_strategies": len(mock_strategies),
            "successful_batches": 1,
            "evolution_time_seconds": 10.0,
            "strategies": mock_strategies
        }
        
        evolution_system.evolved_strategies = mock_strategies
        
        # Step 2: Serialize and load strategies
        config_results = await evolution_system._serialize_and_load_strategies(mock_evolution_results)
        
        assert config_results["top_loaded"] > 0
        top_strategies = config_results.get("top_strategies", mock_strategies[:3])
        
        logger.info(f"âœ… Strategy serialization: {len(top_strategies)} strategies available")
        
        # Step 3: Validate strategies
        validation_results = await validation_pipeline.validate_strategies(
            strategies=top_strategies,
            validation_mode="minimal",
            time_limit_hours=0.05,  # 3 minutes
            concurrent_limit=2
        )
        
        assert validation_results["strategies_validated"] > 0
        
        # Extract validated strategies (simulate successful validation)
        validated_strategies = top_strategies[:2]  # Take first 2 as validated
        
        logger.info(f"âœ… Strategy validation: {len(validated_strategies)} strategies validated")
        
        # Step 4: Deploy strategies
        deployment_results = await deployment_manager.deploy_strategies(
            strategies=validated_strategies,
            deployment_mode=DeploymentMode.PAPER_TRADING
        )
        
        assert deployment_results["strategies_considered"] == len(validated_strategies)
        
        logger.info(f"âœ… Strategy deployment: {deployment_results['strategies_deployed']} strategies deployed")
        
        # Step 5: Monitor system health
        health_snapshot = await health_monitor.get_system_health()
        
        assert health_snapshot.overall_status in [HealthStatus.EXCELLENT, HealthStatus.GOOD, HealthStatus.WARNING]
        
        logger.info(f"âœ… System health check: {health_snapshot.overall_status.value}")
        
        # Step 6: Test resilience coordination
        async def integration_operation():
            return {
                "evolution_strategies": len(mock_strategies),
                "validated_strategies": len(validated_strategies),
                "deployed_strategies": deployment_results["strategies_deployed"],
                "system_health": health_snapshot.overall_score
            }
        
        result, success = await resilience_manager.execute_with_resilience(
            operation=integration_operation,
            operation_name="cross_component_integration",
            failure_type=FailureType.SYSTEM_OVERLOAD
        )
        
        assert success is True
        assert result["evolution_strategies"] > 0
        
        logger.info("âœ… Cross-component resilience integration successful")
        
        # Verify end-to-end data flow
        final_summary = {
            "total_evolved": len(mock_strategies),
            "total_validated": len(validated_strategies),
            "total_deployed": deployment_results["strategies_deployed"],
            "system_health_score": health_snapshot.overall_score,
            "integration_success": success
        }
        
        logger.info(f"ğŸ‰ Cross-Component Integration Summary: {final_summary}")
        
        # Assert end-to-end pipeline success
        assert final_summary["total_evolved"] > 0
        assert final_summary["total_validated"] >= 0
        assert final_summary["integration_success"] is True
        assert final_summary["system_health_score"] > 0
        
        logger.info("ğŸ‰ Cross-Component Integration tests passed")
    
    async def test_performance_and_scalability(self, settings):
        """Test performance and scalability characteristics."""
        
        logger.info("ğŸ§ª Testing Performance and Scalability")
        
        # Create larger dataset for performance testing
        large_strategy_set = [
            MockStrategy(fitness_score=1.5 + i * 0.1, config_name=f"perf_test_strategy_{i}")
            for i in range(10)  # 10 strategies for performance test
        ]
        
        # Test validation pipeline performance
        validation_pipeline = TripleValidationPipeline(settings=settings)
        
        start_time = time.time()
        validation_results = await validation_pipeline.validate_strategies(
            strategies=large_strategy_set,
            validation_mode="minimal",
            time_limit_hours=0.1,
            concurrent_limit=5
        )
        validation_time = time.time() - start_time
        
        # Performance assertions
        strategies_per_second = validation_results["strategies_validated"] / max(validation_time, 0.1)
        
        assert strategies_per_second > 0.5  # At least 0.5 strategies per second
        assert validation_time < 300  # Should complete within 5 minutes
        
        logger.info(f"âœ… Validation performance: {strategies_per_second:.2f} strategies/second")
        
        # Test deployment manager performance
        deployment_manager = StrategyDeploymentManager(settings=settings)
        
        start_time = time.time()
        deployment_results = await deployment_manager.deploy_strategies(
            strategies=large_strategy_set[:5],  # Deploy 5 strategies
            deployment_mode=DeploymentMode.PAPER_TRADING
        )
        deployment_time = time.time() - start_time
        
        deployments_per_second = deployment_results["strategies_considered"] / max(deployment_time, 0.1)
        
        assert deployments_per_second > 0.1  # At least 0.1 deployments per second
        assert deployment_time < 180  # Should complete within 3 minutes
        
        logger.info(f"âœ… Deployment performance: {deployments_per_second:.2f} deployments/second")
        
        # Test health monitoring performance
        health_monitor = SystemHealthMonitor(settings=settings)
        
        start_time = time.time()
        health_snapshot = await health_monitor.get_system_health()
        health_check_time = time.time() - start_time
        
        assert health_check_time < 10  # Health check should complete within 10 seconds
        assert len(health_snapshot.components) > 0
        
        logger.info(f"âœ… Health check performance: {health_check_time:.2f}s")
        
        logger.info("ğŸ‰ Performance and Scalability tests passed")
    
    async def test_error_handling_and_recovery(self, settings):
        """Test error handling and recovery mechanisms."""
        
        logger.info("ğŸ§ª Testing Error Handling and Recovery")
        
        # Test resilience manager error handling
        resilience_manager = ResilienceManager(settings=settings)
        
        # Test operation that raises an exception
        async def failing_operation():
            raise ValueError("Simulated operation failure")
        
        result, success = await resilience_manager.execute_with_resilience(
            operation=failing_operation,
            operation_name="failing_test_operation",
            failure_type=FailureType.COMPUTATION_FAILURE
        )
        
        # Should handle failure gracefully
        assert success is False
        assert result is None
        
        # Check that failure was recorded
        failure_summary = resilience_manager.get_failure_summary(hours_back=1)
        assert failure_summary["total_failures"] > 0
        
        logger.info("âœ… Resilience manager error handling successful")
        
        # Test validation pipeline error recovery
        validation_pipeline = TripleValidationPipeline(settings=settings)
        
        # Test with invalid strategy (None)
        invalid_strategies = [None]
        
        try:
            validation_results = await validation_pipeline.validate_strategies(
                strategies=invalid_strategies,
                validation_mode="minimal",
                time_limit_hours=0.02
            )
            
            # Should handle gracefully without crashing
            assert validation_results["strategies_validated"] >= 0
            assert validation_results["strategies_completed"] >= 0
            
        except Exception as e:
            # If exception occurs, it should be a controlled failure
            logger.info(f"Validation pipeline handled exception gracefully: {e}")
        
        logger.info("âœ… Validation pipeline error recovery successful")
        
        # Test deployment manager error handling
        deployment_manager = StrategyDeploymentManager(settings=settings)
        
        # Test with empty strategy list - API now consistent
        deployment_results = await deployment_manager.deploy_strategies(
            strategies=[],
            deployment_mode=DeploymentMode.PAPER_TRADING
        )
        
        # Should handle empty list gracefully with consistent structure
        assert deployment_results["strategies_deployed"] == 0
        assert deployment_results["strategies_considered"] == 0
        assert deployment_results["strategies_selected"] == 0  # Now included
        assert deployment_results["deployment_failures"] == 0  # Now included
        assert "resource_allocation" in deployment_results  # Now included
        
        logger.info("âœ… Deployment manager error handling successful")
        
        logger.info("ğŸ‰ Error Handling and Recovery tests passed")


# Performance benchmark for production readiness assessment
async def benchmark_phase3_performance():
    """Benchmark Phase 3 system for production readiness."""
    
    print("\n" + "="*80)
    print("ğŸ PHASE 3 PERFORMANCE BENCHMARK")
    print("="*80)
    
    settings = get_settings()
    
    # Create test dataset
    benchmark_strategies = [
        MockStrategy(fitness_score=2.0 + i * 0.2, config_name=f"benchmark_strategy_{i}")
        for i in range(20)
    ]
    
    # Benchmark validation pipeline
    print("ğŸ“Š Benchmarking Validation Pipeline...")
    validation_pipeline = TripleValidationPipeline(settings=settings)
    
    start_time = time.time()
    validation_results = await validation_pipeline.validate_strategies(
        strategies=benchmark_strategies[:10],
        validation_mode="fast",
        time_limit_hours=0.2,
        concurrent_limit=5
    )
    validation_duration = time.time() - start_time
    
    validation_throughput = validation_results["strategies_validated"] / max(validation_duration, 0.1)
    
    print(f"   âœ… Validated {validation_results['strategies_validated']} strategies in {validation_duration:.1f}s")
    print(f"   ğŸ“ˆ Throughput: {validation_throughput:.2f} strategies/second")
    print(f"   âš¡ Success Rate: {validation_results['completion_rate']:.1%}")
    
    # Benchmark deployment system
    print("ğŸš€ Benchmarking Deployment System...")
    deployment_manager = StrategyDeploymentManager(settings=settings)
    
    start_time = time.time()
    deployment_results = await deployment_manager.deploy_strategies(
        strategies=benchmark_strategies[:8],
        deployment_mode=DeploymentMode.PAPER_TRADING
    )
    deployment_duration = time.time() - start_time
    
    deployment_throughput = deployment_results["strategies_considered"] / max(deployment_duration, 0.1)
    
    print(f"   âœ… Processed {deployment_results['strategies_considered']} strategies in {deployment_duration:.1f}s")
    print(f"   ğŸ“ˆ Throughput: {deployment_throughput:.2f} strategies/second")
    print(f"   ğŸ¯ Deployed: {deployment_results['strategies_deployed']} strategies")
    
    # Benchmark health monitoring
    print("ğŸ¥ Benchmarking Health Monitor...")
    health_monitor = SystemHealthMonitor(settings=settings)
    
    start_time = time.time()
    health_snapshot = await health_monitor.get_system_health()
    health_duration = time.time() - start_time
    
    print(f"   âœ… Health check completed in {health_duration:.2f}s")
    print(f"   ğŸ’š System Status: {health_snapshot.overall_status.value}")
    print(f"   ğŸ“Š Health Score: {health_snapshot.overall_score:.3f}")
    print(f"   ğŸ”§ Components Checked: {len(health_snapshot.components)}")
    
    print("="*80)
    print("ğŸ‰ PHASE 3 BENCHMARK COMPLETE - PRODUCTION READY")
    print("="*80)


if __name__ == "__main__":
    """Run integration tests and benchmarks."""
    
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    print("ğŸš€ Starting Phase 3 Ultra-Compressed Evolution Integration Tests")
    
    # Run benchmark
    asyncio.run(benchmark_phase3_performance())
    
    # Run pytest suite
    import sys
    sys.exit(pytest.main([__file__, "-v", "--tb=short"]))